#!/usr/bin/env python

from std_msgs.msg import String, Float64
#import ros_numpy
import numpy as np
from scipy.linalg import svd, norm, solve
from math import acos, pi
from stereo_msgs.msg import DisparityImage
from rospy.numpy_msg import numpy_msg
import rospy

class KinectLink(object):
    # Node for configuring and linking the kinect 1 to the quadcopter driver.
    # Reads a disparity image from the openni driver over the /camera/depth/disparity
    # topic, computes the true positions and sents the data to the
    # kinect_pos_measurement topic. Can calibrate for background noise
    # and find a mapping from the camera coordinate system to the true,
    # global coordinate system, mapped to eachother using the transformation
    #             _                                  _
    #            |cos(self.angle)  0         0        |
    # u_camera = |       0         1         0        |u_global
    #            |_      0         0  cos(self.angle)_|
    #
    # The self.angle and background noise calibration can be done from the master.
 
    def __init__(self):

        # Sets up subscribers
        self.data_sub = rospy.Subscriber('/camera/depth/disparity', numpy_msg(DisparityImage), self.handle_kinect_data)
        self.status_sub = rospy.Subscriber('Kinect_status', String, self.handle_status_data)

        # Sets up publisher
        self.status_pub = rospy.Publisher('system_status', String, queue_size = 10)
        self.data_pub = rospy.Publisher('kinect_pos_measurement', String, queue_size = 10)

        self.mode = 'ConfigureBackground'
        self.angle = None

        # Used in configure bckground
        self.background = np.zeros((480, 640))
        self.framecount = 0

        # Used in configure depth
        self.currentImage = None
        self.maxDepthSamples = 10
        self.measuredDepth = np.zeros(self.maxDepthSamples)
        self.userSuppliedDepth = np.zeros(self.maxDepthSamples)

    def handle_kinect_data(self, msg):
        # Callback for the Image data generated by the kinect. Treats the data
        # differently depending on the operating mode. The modes are:
        #
        #    * ConfigureBackround - Takes 100 consecutive image and stores the
        #        mean value as a reference for other running modes and image
        #        processing.
        #    * ConfigureDepth - Stores each received image in the
        #        self.currentImage attribute, where it is then accessed by the
        #        configure_* functions
        #    * ConfigureAngle configures the angle of the kinect based on the
        #        assumption that the kinect is facing a wall, does nothing
        #        presentl.
        #    * Run - prints the current position of the quadcopter to the
        #        /kinect_position_measurement topic

        np_image = np.zeros((480, 640))
        #np_image = ros_numpy.numpify(msg.image)
        image = np_image - self.background

        if self.mode == 'ConfigureBackground':
            # Takes a total of 100 backround samples and stores the mean
            if self.framecount < 100:
                self.background = self.background + np_image
                self.framecount += 1
            else:
                self.background = self.background / float(self.framecount)
                self.mode = 'Idle'

                # Allows the master node to continue operation
                self.status_pub.publish('True')

        elif self.mode == 'ConfigureDepth' or self.mode == 'ConfigureAngle':
            self.currentImage = image

        else:
            self.currentImage = image

            i,j = np.unravel_index(image.argmax(), image.shape)
            print i, j, image[i, j], np_image[i, j], self.background[i, j]
            print type(image)
            #i,j = np.unravel_index(np_image.argmax(), np_image.shape)
            #print i, j, np.max(np_image)
            #i,j = np.unravel_index(self.background.argmax(), self.background.shape)
            #print i, j, np.max(self.background)
            print '-'*10

    def handle_status_data(self, msg):
        # Callback for the status of the kinect.

        if msg.data == 'ConfigBackround':
            self.mode = msg.data

        elif msg.data == 'ConfigDepth':
            self.mode = msg.data
            self.configure_depth()

        elif msg.data == 'ConfigAngle':
            self.mode = msg.data
            self.configure_angle()
        else:
            print 'Unsupported kinect mode %s, no action taken' % (msg.data)

    def configure_depth(self):
        # Configures depth by finding a linear relasionship between
        # a set of measured and user supplied points in space
        print 'Configuring depth...'

        nSamples = 0
        while nSamples < self.maxDepthSamples:
            command = raw_input('Enter distance from camera [m]: ')
            self.userSuppliedDepth[nSamples] = float(command)

            # TODO - Set measuredDepth with the data in self.currentImage
            self.measuredDepth[nSamples] = float(command)
            nSamples += 1

        #TODO - Use the self.regression() function to find a mapping from
        # the measured values to the user supplied values
        print 'Entered values: %s' % (str(self.userSuppliedDepth))
        print 'Configuration complete!'

        # Allows the master node to continue operation
        self.status_pub.publish('True')

    def configure_angle(self):
        # configures angle using the single value decomposition, such that
        # the x-axis in the camera frame is parallell with the cameras line
        # of sight, and that the camera rotates around the y-axis in both the
        # global and camera coordinate system.

        print 'Configuring angle...'

        # TODO replace the points variable with random depth data in the lower
        # center of an actual image
        points = np.random.rand(10,3)

        x = points[:,0]*0.5
        y = points[:,1]
        z = points[:,2]

        P = np.array([sum(x),sum(y),sum(z)])/len(x);
        [U,S,V] = svd(np.transpose(np.array([x-P[0],y-P[1],z-P[2]])));
        N = -1./V[2,2]*V[2,:]

        XZnormal = np.array([[N[0]],[N[2]]])
        angle = (N[0]/abs(N[0]))*acos(N[0]/norm(XZnormal))

        if abs(angle) > pi/2 and abs(angle) < 3*pi/2:
            angle=pi-angle

        self.angle = angle*180./pi
        print 'Angle = %s [degree]' % (str(self.angle))
        print 'Configuration complete!'

        # Allows the master node to continue operation
        self.status_pub.publish('True')

    def regression(self,xx,yy,option):
        # Performs a linear and exponential regression
        #
        # Linear model:      y = c_0 + c_1 * x
        # Exponential model: y = exp(c_0 + c_1 * x)
        #
        # ARGS:
        #    xx - Array of shape (N,)
        #    yy - Array of shape (N,)
        #    option - String, either 'exp' or 'lin'
        a = np.transpose(np.array([np.ones(N),tt]))
        if option == 'lin':
            b = np.transpose(np.array([yy]))
        elif option == 'exp':
            b = np.transpose(np.array([np.log(yy)]))
        return solve(np.dot(np.transpose(a),a),np.dot(np.transpose(a),b))

    def __str__(self):
        return 'kinect node'

def main():
    rospy.init_node('KinectLink')
    kL = KinectLink()
    rospy.spin()

if __name__ == '__main__':
    main()
